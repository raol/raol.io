<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <title>Machine learning with python. Linear regression. Part 2 &middot; </title>
        <meta name="description" content="In the first article on linear regression I promised to show how to do it better, 
so this post will be about truly scientific approach to the problem. Don&rsquo;t worry if you 
don&rsquo;t get it off hand. Honestly speaking it took some time for me to figure out what&rsquo;s going on
and even now from time to time I take some paper and draw matrices/vectors to be sure I&rsquo;m doing 
everything right.">
        <meta name="HandheldFriendly" content="True">
        <meta name="MobileOptimized" content="320">
        <meta name="generator" content="Hugo 0.88.1" />
        <meta name="robots" content="index,follow">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <link rel="stylesheet" href="https://raol.io/dist/site.css">
        <link rel="stylesheet" href="https://raol.io/dist/syntax.css">
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,400,600,700,300&subset=latin,cyrillic-ext,latin-ext,cyrillic">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha384-wvfXpqpZZVQGK6TAh5PVlGOfQNHSoD2xbE+QkPxCAFlNEevoEH3Sl0sibVcOQVnN" crossorigin="anonymous">
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
                tex2jax: {inlineMath: [['$','$']]}
            });
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

        
        
        

    </head>
    <body>
        

        <div id="wrapper">
            <header class="site-header">
                <div class="container">
                    <div class="site-title-wrapper">
                        
                            <h1 class="site-title">
                                <a href="https://raol.io/">Raol.io</a>
                            </h1>
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Twitter" aria-label="Twitter" href="https://twitter.com/raoldev" rel="me" >
                                <i class="fa fa-twitter" aria-hidden="true"></i>
                            </a>
                        
                        
                        
                        
                            <a class="button-square button-social hint--top" data-hint="Github" aria-label="Github" href="https://github.com/raol" rel="me">
                                <i class="fa fa-github-alt" aria-hidden="true"></i>
                            </a>
                        
                        
                        
                        
                    </div>

                    <ul class="site-nav">
                        

                    </ul>
                </div>
            </header>

            <div id="container">


<div class="container">
    <article class="post-container" itemscope="" itemtype="http://schema.org/BlogPosting">
        <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Machine learning with python. Linear regression. Part 2</h1>
    
    <p class="post-date post-line">
        <span>Published <time datetime="2014-04-22" itemprop="datePublished">Tue, Apr 22, 2014</time></span>
        <span>by</span>
        <span itemscope="" itemprop="author" itemtype="https://schema.org/Person">
            <span itemprop="name">
                <a href="#" itemprop="url" rel="author"></a>
            </span>
        </span>
    </p>
    
</header>

        <div class="post-content clearfix" itemprop="articleBody">
    

    <p>In the first article on linear regression I promised to show how to do it better, 
so this post will be about truly scientific approach to the problem. Don&rsquo;t worry if you 
don&rsquo;t get it off hand. Honestly speaking it took some time for me to figure out what&rsquo;s going on
and even now from time to time I take some paper and draw matrices/vectors to be sure I&rsquo;m doing 
everything right.</p>
<p>First let&rsquo;s consider fairly simple example. Suppose you were given following column vector:
$$ 
\vec{X} = 
\begin{bmatrix} 
x_1 \\\<br>
x_2 \\\<br>
\vdots \\\<br>
x_n
\end{bmatrix}
$$</p>
<p>and you need to find sum of $ x_{n}^{2} $. I understand that it&rsquo;s trivial and synthetic example, but later 
all these over-complications will come handy.</p>
<p>First that that came in mind is just to make iteration over $ x_n $ and accumulate values like below.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">print sum([x<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#66d9ef">for</span> x <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>)])
</code></pre></div><p>but that&rsquo;s too easy and not cool at all. 
We&rsquo;re going to use the power of linear algebra here. Fortunately I took linear algebra class in the university
and it&rsquo;s good time to resurrect the knowledge and give it a chance. <a href="http://en.wikipedia.org/wiki/Matrix_multiplication">Matrix multiplication</a>
works well here.</p>
<p>So to get sum of squared vector elements we will use following formula
$$ 
\text{sum} = 
\begin{bmatrix} 
x_1 &amp;
x_2 &amp;
\cdots &amp;
x_n
\end{bmatrix}
*
\begin{bmatrix} 
x_1 \\\<br>
x_2 \\\<br>
\vdots \\\<br>
x_n
\end{bmatrix} = \vec{X}^T * \vec{X}
$$</p>
<p>Now let&rsquo;s look at our problem from the first post. We have pairs of $ (x^{(i)}, y^{(i)}) $ and hypothesis $ y = \theta_0 + \theta_1*x $
.
If we look at this task from the slightly different angle we will see that we were given two column vectors. One for $ X $ values as input 
and second for $ Y $ values as our output. Let&rsquo;s rewrite our hypothesis in following form to make it even more obvious 
$ y = \theta_0*1 + \theta_1*x $. Clear now? If not, don&rsquo;t worry, below I&rsquo;ll give detailed explanation of all the steps.</p>
<p>It&rsquo;s easy to see (frankly saying it was not so obvious until I was told about it) that formula for $ Y $ looks as matrix multiplication, where $ \theta $ is column vector
$$ \theta = 
\begin{bmatrix} 
\theta_0 \\\ 
\theta_1 
\end{bmatrix} $$
and $ X $ is $ m\times 2 $ matrix, where m is the size of our dataset. So let&rsquo;s rewrite it as</p>
<p>$$ 
h_{\theta}(x) = 
\begin{bmatrix}
1 &amp; x_1^{(1)} \\\<br>
1 &amp; x_1^{(2)} \\\<br>
\vdots &amp; \vdots \\\<br>
1 &amp; x_1^{(m)}
\end{bmatrix}
*
\begin{bmatrix}
\theta_0 \\\<br>
\theta_1
\end{bmatrix}
= \begin{bmatrix}
y^{(1)} \\\<br>
y^{(2)} \\\<br>
\vdots \\\<br>
y^{(m)}
\end{bmatrix}
$$
And let&rsquo;s call all those ones as $ x_0^{(i)} $
So cost function can be rewritten in the the following vectorized form
$$ J(\theta) = \frac{1}{2m}\sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)})^2 = \frac{1}{2m}\text{sum}((X*\theta - Y)^2) $$</p>
<p>Code for partial derivatives of $ J(\theta) $ function can be also vectorized. It was a brain teaser to 
write all $ \theta_j $ update in one step but when it&rsquo;s done it seems really obvious.</p>
<p>We have vectorized form of $ h_{\theta}(X) - Y $ and to calculate all partial derivatives that are now written in form
$$ \frac{\partial}{\partial \theta_j} J(\theta_j) = \frac{1}{m}\sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)})x_{j}^{(i)} $$ 
since $ x_{0}^{(i)} = 1 $
Our $ \theta $ is a column vector so to decrease it we have to have the same column vector for its gradient.</p>
<p>Dimension of $ h_{\theta}(X) - Y $ is $ m\times 1 $ and dimensions of $ X $ with $ x_0 $ column added is $ m\times 2 $, so to get 
$ \sum_{i=0}^{n} (h_\theta(x^{(i)}) - y^{(i)})x_{j}^{(i)} $ we can transpose our $ X $ matrix to $ 2\times m $ one and multiply it
with the $ h_{\theta}(X) - Y $ subtraction resulting vector. Below is the full formula for gradient descent
$$ \vec{\theta} = \vec{\theta} - \frac{\alpha}{m}X^T*(h_\theta(X) - Y) $$</p>
<p>And that&rsquo;s really it. We have our function parameters updated in one step. There are lots of advantages of the vectorized implementation. 
You don&rsquo;t need to bother with all those for loops when you have $ X $ with multiple features (here we have just one, but in next posts we will 
deal with multiple features and polynomial regression). Second as all calculations are rarely done by hand, more often by some 
libraries such as <code>numpy</code> in python, they will be faster because all libraries are highly optimized and use operations parallelism, thus
vectorized approach will run much faster.</p>
<p>And below is the code for the vectorized implementation.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">costFunction</span>(theta, x, y):
    <span style="color:#66d9ef">return</span> (sum((x<span style="color:#f92672">.</span>dot(theta) <span style="color:#f92672">-</span> y) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> x<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]))[<span style="color:#ae81ff">0</span>]

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gradientDescent</span>(theta, x, y):
    m <span style="color:#f92672">=</span> y<span style="color:#f92672">.</span>size;
    numiter <span style="color:#f92672">=</span> <span style="color:#ae81ff">1500</span>
    alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
    costHistory <span style="color:#f92672">=</span> []
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">0</span>, numiter):
        theta <span style="color:#f92672">=</span> theta <span style="color:#f92672">-</span> (alpha <span style="color:#f92672">/</span> m) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>transpose(x)<span style="color:#f92672">.</span>dot(x<span style="color:#f92672">.</span>dot(theta) <span style="color:#f92672">-</span> y)
        costHistory<span style="color:#f92672">.</span>append(costFunction(theta, x, y))
    <span style="color:#66d9ef">return</span> costHistory, theta

data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>loadtxt(<span style="color:#e6db74">&#39;ex1data1.txt&#39;</span>, delimiter<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;,&#39;</span>)

X <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">0</span>]
Y <span style="color:#f92672">=</span> data[:, <span style="color:#ae81ff">1</span>]
Y <span style="color:#f92672">=</span> Y<span style="color:#f92672">.</span>reshape((Y<span style="color:#f92672">.</span>size, <span style="color:#ae81ff">1</span>))
m <span style="color:#f92672">=</span> X<span style="color:#f92672">.</span>size

t <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(shape<span style="color:#f92672">=</span>(m, <span style="color:#ae81ff">2</span>))
t[:, <span style="color:#ae81ff">1</span>] <span style="color:#f92672">=</span> X
X <span style="color:#f92672">=</span> t;
theta <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>])
theta <span style="color:#f92672">=</span> theta<span style="color:#f92672">.</span>reshape((<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>))

costHistory, theta <span style="color:#f92672">=</span> gradientDescent(theta, X, Y)

plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Iterations count&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;J($</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">theta$) value&#34;</span>)
plt<span style="color:#f92672">.</span>axis([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1500</span>, min(costHistory), max(costHistory)])

plt<span style="color:#f92672">.</span>plot(range(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1500</span>), costHistory)

plt<span style="color:#f92672">.</span>show()
</code></pre></div><p>Since function the same as in the first post, I won&rsquo;t show it here. Instead let&rsquo;s look at gradient work.
We will see how cost function $ J(\theta) $ is getting lower and lower with each step of gradient descent.</p>
<p><img src="/assets/images/linear_regression_2/theta_minimize.png" alt="Joftheta minimize"></p>
</div>

        <footer class="post-footer clearfix">
        <p class="post-tags">
            <span>Tagged:</span>
                <a href="/tags/python/">python</a>, 
                <a href="/tags/machine-learning/">machine learning</a>
        </p>
    <div class="share">
    </div>
</footer>

        
    </article>
</div>

            </div>
        </div>

        <footer class="footer">
            <div class="container">
                <div class="site-title-wrapper">
                    <h1 class="site-title">
                        <a href="https://raol.io/">Raol.io</a>
                    </h1>
                    <a class="button-square button-jump-top js-jump-top" href="#" aria-label="Back to Top">
                        <i class="fa fa-angle-up" aria-hidden="true"></i>
                    </a>
                </div>

                <p class="footer-copyright">
                    <span>&copy; 2017 / Powered by <a href="https://gohugo.io/">Hugo</a></span>
                </p>
                <p class="footer-copyright">
                    <span><a href="https://github.com/roryg/ghostwriter">Ghostwriter theme</a> By <a href="http://jollygoodthemes.com">JollyGoodThemes</a></span>
                    <span>/ <a href="https://github.com/jbub/ghostwriter">Ported</a> to Hugo By <a href="https://github.com/jbub">jbub</a></span>
                </p>
            </div>
        </footer>

        <script src="https://raol.io/js/jquery-1.11.3.min.js"></script>
        <script src="https://raol.io/js/jquery.fitvids.js"></script>
        <script src="https://raol.io/js/scripts.js"></script>
    </body>
</html>

